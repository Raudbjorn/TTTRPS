version: '3.8'

services:
  # =============================================================================
  # Development Services
  # =============================================================================
  
  # Development environment with full tooling
  mbed-dev:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: mbed-dev
    volumes:
      - .:/app
      - mbed-data:/data
      - mbed-outputs:/outputs
      - mbed-cache:/app/.mbed
    environment:
      - MBED_HARDWARE=auto
      - MBED_LOG_LEVEL=DEBUG
      - MBED_DB_PATH=/outputs/database
    working_dir: /app
    command: /bin/bash
    stdin_open: true
    tty: true
    profiles:
      - dev

  # =============================================================================
  # Production Services
  # =============================================================================
  
  # CPU-only production service
  mbed-cpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: cpu-only
    container_name: mbed-cpu
    volumes:
      - ./data:/data:ro
      - mbed-outputs:/outputs
      - mbed-cache:/app/.mbed
    environment:
      - MBED_HARDWARE=cpu
      - MBED_WORKERS=8
      - MBED_BATCH_SIZE=64
      - MBED_LOG_LEVEL=INFO
      - MBED_DB_PATH=/outputs/database
    restart: unless-stopped
    profiles:
      - cpu
      - production

  # CUDA GPU production service
  mbed-cuda:
    build:
      context: .
      dockerfile: Dockerfile
      target: cuda-production
    container_name: mbed-cuda
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./data:/data:ro
      - mbed-outputs:/outputs
      - mbed-cache:/app/.mbed
    environment:
      - MBED_HARDWARE=cuda
      - MBED_MIXED_PRECISION=true
      - MBED_MULTI_GPU=true
      - MBED_CUDA_VRAM_RESERVED_GB=2.0
      - MBED_BATCH_SIZE=256
      - MBED_WORKERS=4
      - MBED_LOG_LEVEL=INFO
      - MBED_DB_PATH=/outputs/database
    restart: unless-stopped
    profiles:
      - cuda
      - gpu
      - production

  # =============================================================================
  # Database Services (Optional)
  # =============================================================================
  
  # PostgreSQL with pgvector for production vector storage
  postgres:
    image: pgvector/pgvector:pg16
    container_name: mbed-postgres
    environment:
      - POSTGRES_DB=mbed_vectors
      - POSTGRES_USER=mbed
      - POSTGRES_PASSWORD=mbed_secure_password
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./config/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    profiles:
      - postgres
      - database
      - production

  # Qdrant vector database service
  qdrant:
    image: qdrant/qdrant:v1.7.0
    container_name: mbed-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    profiles:
      - qdrant
      - database
      - production

  # =============================================================================
  # Monitoring and Management
  # =============================================================================
  
  # Web UI for monitoring (optional)
  # Note: For production deployments, consider using nginx or a dedicated monitoring solution
  mbed-monitor:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: mbed-monitor
    volumes:
      - mbed-outputs:/outputs:ro
      - mbed-cache:/app/.mbed:ro
    environment:
      - MBED_LOG_LEVEL=INFO
    # Using uvicorn for production-ready serving
    # For true production, consider nginx or dedicated monitoring stack
    command: >
      sh -c "pip install uvicorn fastapi &&
             python -c 'from fastapi import FastAPI; from fastapi.staticfiles import StaticFiles; import uvicorn; app = FastAPI(); app.mount(\"/\", StaticFiles(directory=\"/outputs\", html=True), name=\"static\"); uvicorn.run(app, host=\"0.0.0.0\", port=8000)'"
    ports:
      - "8000:8000"
    profiles:
      - monitor

  # =============================================================================
  # Batch Processing Service
  # =============================================================================
  
  # Batch processing service for large-scale operations
  mbed-batch:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: mbed-batch
    volumes:
      - ./data:/data:ro
      - mbed-outputs:/outputs
      - mbed-cache:/app/.mbed
      - ./batch-config.env:/app/.env:ro
    environment:
      - MBED_HARDWARE=auto
      - MBED_LOG_LEVEL=INFO
      - MBED_DB_PATH=/outputs/database
      - MBED_BATCH_SIZE=512
      - MBED_WORKERS=16
    command: |
      sh -c "
        echo 'Starting batch processing...'
        ./mbed generate /data --verbose --output /outputs
      "
    profiles:
      - batch

volumes:
  mbed-data:
    driver: local
  mbed-outputs:
    driver: local
  mbed-cache:
    driver: local
  postgres-data:
    driver: local
  qdrant-data:
    driver: local

networks:
  default:
    name: mbed-network